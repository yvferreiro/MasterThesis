{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yolandaferreirofranchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from genderize import Genderize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#name_probability_list = pd.read_pickle(r'C:\\Users\\DanielleDuncan\\Documents\\Masters-Thesis\\name_probability_list.pickle')\n",
    "name_probability_list = pd.read_pickle(r\"/Users/yolandaferreirofranchi/Documents/GitHub/Masters-Thesis/name_probability_list.pickle\")\n",
    "#df_09 = pd.read_pickle(r'C:\\Users\\DanielleDuncan\\Documents\\Masters-Thesis\\2009_preprocessed_date.pickle')\n",
    "df_09 = pd.read_pickle(r\"/Users/yolandaferreirofranchi/Documents/GitHub/Masters-Thesis/2009_preprocessed_date.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion json file\n",
    "\n",
    "#filejson = r\"C:\\Users\\danie\\Desktop\\bbc_latest_news_dataset_2021.json\"\n",
    "\n",
    "#article_df_4 = pd.read_json(filejson)\n",
    "#article_df_4.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- break down the dataset files into years\n",
    "- remove the name portion and have it seperate\n",
    "- add two save points for preprocessed and training data\n",
    "- add threading (safe list!!!!)\n",
    "- finish the paper section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingestion\n",
    "\n",
    "#Actual Dataset--File 1\n",
    "\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_01.csv\"\n",
    "article_df_1 = pd.read_csv(filecsv)\n",
    "\n",
    "#Actual Dataset--File 2\n",
    "\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_02.csv\"\n",
    "article_df_2 = pd.read_csv(filecsv)\n",
    "\n",
    "#Actual Dataset--File 3\n",
    "filecsv = r\"C:\\Users\\danie\\Desktop\\bbc_news_03.csv\"\n",
    "article_df_3 = pd.read_csv(filecsv)\n",
    "\n",
    "#Merge Datasets\n",
    "\n",
    "article_df = pd.concat([article_df_1, article_df_2, article_df_3])\n",
    "article_df = article_df.assign(Article_Number=range(len(article_df)))\n",
    "article_df = article_df.reset_index()\n",
    "article_df.drop([\"publisher\", \"header_image\", \"index\", \"raw_description\", \"short_description\", \"uniq_id\", \"scraped_at\"], axis=1)\n",
    "year = article_df['published_at'].str[:4]\n",
    "article_df['year']=year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences in an article \n",
    "\n",
    "def split_sentences(article, article_id, year):\n",
    "    pattern = r'(?<=[a-z0-9\"]) *[.?!] *(?=[A-Z])'\n",
    "    article = re.sub(pattern, r'\\g<0> ', article)\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    sentences_with_id = [(sentence, article_id, year) for sentence in sentences]\n",
    "    return sentences_with_id\n",
    "\n",
    "sentences_list = []\n",
    "\n",
    "# add sentences to a new DF along with article ID \n",
    "for article, article_id, year in article_df[['description','Article_Number', 'year']].values:\n",
    "    sentences = split_sentences(str(article), article_id, year)\n",
    "    sentences_list.extend(sentences)\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_list, columns= ['sentences', 'article_id', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12 = sentences_df[sentences_df['year'] == '2012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing (sentence):\n",
    "    Male_count = 0\n",
    "    Female_count = 0\n",
    "    APIcallfail= 0\n",
    "\n",
    "#regex_cleanup\n",
    "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence)\n",
    "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
    "    sentence = re.sub(r'&amp;', '', sentence) \n",
    "    sentence = re.sub(\"\\d+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
    "    sentence = re.sub(r'<br />', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\b's\\b\", '', sentence)\n",
    "\n",
    "#tokenize\n",
    "    sentence =  nltk.TweetTokenizer().tokenize(sentence)\n",
    "\n",
    "#remove small words\n",
    "    sentence = [ x for x in sentence if len(x) > 2 ]\n",
    "\n",
    "#tag_and_stem\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence)\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    pn_tags = {'NNP', 'NNPS'}\n",
    "\n",
    "    new_words = []\n",
    "    proper_nouns = []\n",
    "\n",
    "    for word, tag in tagged_sentence: \n",
    "        if tag not in pn_tags: \n",
    "            if tag.startswith(\"V\"):\n",
    "                lemmas = lemma.lemmatize(word, \"v\")\n",
    "            else: \n",
    "                lemmas = lemma.lemmatize(word)\n",
    "            new_words.append((lemmas))\n",
    "        else:\n",
    "            proper_nouns.append([word, tag])\n",
    "\n",
    "    sentence = new_words\n",
    "\n",
    "#name_gender\n",
    "    #nltk_results = ne_chunk(tagged_sentence)\n",
    "    nltk_results = ne_chunk(proper_nouns)\n",
    "\n",
    "    for nltk_result in nltk_results:\n",
    "        if type(nltk_result) == Tree:\n",
    "            name = ''\n",
    "            for nltk_result_leaf in nltk_result.leaves():\n",
    "                name += nltk_result_leaf[0] + ' '\n",
    "            if nltk_result.label() == \"PERSON\":\n",
    "                name = name.split(' ')[0]\n",
    "                try: \n",
    "                    word_gender = name_probability_list.get(name)\n",
    "                    if word_gender is None:\n",
    "                        word_gender = Genderize().get1(name).get('gender')\n",
    "                        name_probability_list[name] = word_gender\n",
    "                    if word_gender == 'male':\n",
    "                        Male_count += 1\n",
    "                    if word_gender== 'female':\n",
    "                        Female_count += 1\n",
    "                except Exception as exception:\n",
    "                    APIcallfail +=1\n",
    "            else: \n",
    "                sentence.append(name.strip()) #add a tokenize\n",
    "    \n",
    "#Lower\n",
    "    sentence = [x.lower() for x in sentence]\n",
    "\n",
    "#contractions\n",
    "    new_text = []\n",
    "    for word in sentence:\n",
    "        contraction = contractions.get(word)\n",
    "        if contraction is None:\n",
    "            new_text.append(word)\n",
    "        else:\n",
    "            for word in contraction.split():\n",
    "                new_text.append(word)\n",
    "\n",
    "    sentence = new_text\n",
    "\n",
    "#gendered_count\n",
    "    for w in sentence:\n",
    "        if w in male_list:\n",
    "            Male_count += 1\n",
    "        if w in female_list:\n",
    "            Female_count += 1\n",
    "\n",
    "#remove_stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    sentence = [x for x in sentence if not x in stops]\n",
    "\n",
    "#remove_leakage\n",
    "    new_sent = [x for x in sentence if x not in male_list]\n",
    "    new_sent = [x for x in new_sent if x not in female_list]\n",
    "    sentence = new_sent\n",
    "\n",
    "    print(sentence)\n",
    "    return sentence, Male_count, Female_count, APIcallfail\n",
    "\n",
    "#if still having memory issues we needd to chunk the dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10['encoded_sentences'] = df_10['sentences'].apply(PreProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_09_split = pd.DataFrame(df_09[\"encoded_sentences\"].to_list(), columns=['pre_processed_sent','male_count','female_count','apicall_fail'])\n",
    "result = pd.concat([df_09_split, (df_09.reset_index(drop=True))], axis=1)\n",
    "result = result.drop(\"encoded_sentences\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the api calls!!!\n",
    "with open('name_probability_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(name_probability_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the df\n",
    "with open('2009_preprocessed_date.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_09, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCE LISTS--TO BE CHANGED\n",
    "male_list= {\"abbot\", \"abbots\", \"actor\", \"actors\", \"author\", \"authors\", \"bachelor\", \"bachelors\", \"baron\", \"barons\", \"boy\", \"boys\", \"bridegroom\", \"bridegrooms\", \"brother\", \"brothers\", \"buck\", \n",
    "            \"bucks\", \"conductor\", \"conductors\", \"czar\", \"czars\", \"dad\", \"daddy\", \"daddys\", \"dads\", \"duke\", \"dukes\", \"emperor\", \"emperors\", \"enchanter\", \"enchanters\", \"father\", \"fathers\", \n",
    "            \"gentleman\", \"gentlemans\", \"granddad\", \"granddads\", \"grandfather\", \"grandfathers\", \"grandpa\", \"he\", \"heir\", \"heirs\", \"hero\", \"heros\", \"hes\", \"him\", \"his\", \"host\", \"hosts\", \"hunter\", \n",
    "            \"hunters\", \"husband\", \"husbands\", \"king\", \"kings\", \"landlord\", \"landlords\", \"lord\", \"lords\", \"man\", \"mans\", \"manservant\", \"manservants\", \"master\", \"masters\", \"men\", \"milkman\", \n",
    "            \"milkmans\", \"mister\", \"monk\", \"monks\", \"mr\", \"nephew\", \"nephews\", \"patron\", \"patrons\", \"peacock\", \"peacocks\", \"peer\", \"peers\", \"poet\", \"poets\", \"policeman\", \"policemans\", \"policemen\", \n",
    "            \"policemens\", \"priest\", \"priests\", \"prince\", \"princes\", \"prophet\", \"prophets\", \"shepherd\", \"shepherds\", \"signor\", \"signors\", \"sir\", \"sirs\", \"son\", \"songster\", \"songsters\", \"sons\", \n",
    "            \"stag\", \"stags\", \"stallion\", \"stallions\", \"stepbrother\", \"stepbrothers\", \"stepdad\", \"stepdads\", \"stepfather\", \"stepfathers\", \"steward\", \"stewards\", \"sultan\", \"sultans\", \"traitor\", \n",
    "            \"traitors\", \"uncle\", \"uncles\", \"viscount\", \"viscounts\", \"waiter\", \"waiters\", \"wizard\", \"wizards\"}\n",
    "\n",
    "female_list ={\"abbess\", \"abbesses\", \"actress\", \"actresses\", \"aunt\", \"aunts\", \"authoress\", \"authoresses\", \"baroness\", \"baronesses\", \"benefactress\", \"benefactresses\", \"bride\", \"brides\", \n",
    "              \"conductress\", \"conductresses\", \"countess\", \"countesses\", \"czarina\", \"czarinas\", \"daughter\", \"daughters\", \"duchess\", \"duchesses\", \"empress\", \"empresses\", \"enchantress\", \n",
    "              \"enchantresses\", \"giantess\", \"giantesses\", \"girl\", \"girls\", \"goddess\", \"goddesses\", \"grandma\", \"grandmas\", \"grandmother\", \"grandmothers\", \"heiress\", \"heiresses\", \"hen\", \n",
    "              \"hens\", \"her\", \"heroine\", \"heroines\", \"hers\", \"hostess\", \"hostesses\", \"huntress\", \"huntresses\", \"ladies\", \"lady\", \"landladies\", \"landlady\", \"lioness\", \"lionesses\", \"madam\", \n",
    "              \"madams\", \"maidservant\", \"maidservants\", \"milkmaid\", \"milkmaids\", \"misses\", \"missus\", \"mistress\", \"mistresses\", \"mom\", \"mommy\", \"mommys\", \"moms\", \"mother\", \"mothers\", \"mrs\", \"ms\",\n",
    "                \"mum\", \"mummy\", \"mummys\", \"mums\", \"murderess\", \"murderesses\", \"niece\", \"nieces\", \"nun\", \"nuns\", \"patroness\", \"patronesses\", \"poetess\", \"poetesses\", \"policewoman\", \"policewomen\", \n",
    "                \"priestess\", \"priestesses\", \"princess\", \"princesses\", \"prophetess\", \"prophetesses\", \"queen\", \"queens\", \"she\", \"shepherdess\", \"shepherdesses\", \"shes\", \"signora\", \"signoras\", \n",
    "                \"sister\", \"sisters\", \"songstress\", \"songstresses\", \"spinster\", \"spinsters\", \"stepdaughter\", \"stepdaughters\", \"stepmom\", \"stepmoms\", \"stepmother\", \"stepmothers\", \"stewardess\", \n",
    "                \"stewardesses\", \"sultana\", \"sultanas\", \"temptress\", \"temptresses\", \"tigress\", \"tigresses\", \"traitress\", \"traitresses\", \"viscountess\", \"viscountesses\", \"vixen\", \"vixens\", \n",
    "                \"waitress\", \"waitresses\", \"wife\", \"witch\", \"witches\", \"wive\", \"woman\", \"women\"}\n",
    "\n",
    "#should all gendered words be counted as a gendered thing???\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009    9712\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_1 = result.iloc[0:4875]\n",
    "df_sample_2 = result.iloc[4876::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_2 = df_sample_2.replace('2009','2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(male_col, female_col): \n",
    "    \"\"\"This function compares the count of female to male pronouns. It will output \"1\" if male count bigger\n",
    "    than female count, \"neutral\" if the count is equal, and \"female\" if there is a higher female count. \n",
    "    The function returns strings because we need categorical variables for log reg to run\"\"\"\n",
    "    if female_col > male_col:\n",
    "        return \"1\"\n",
    "    elif male_col > female_col:\n",
    "        return \"0\"\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielleDuncan\\AppData\\Local\\Temp\\ipykernel_14360\\2854444328.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample_1['col_type'] = df_sample_1.apply(lambda row: compare_count(row['male_count'], row['female_count']),axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_sample_1['col_type'] = df_sample_1.apply(lambda row: compare_count(row['male_count'], row['female_count']),axis=1)\n",
    "df_sample_2['col_type'] = df_sample_2.apply(lambda row: compare_count(row['male_count'], row['female_count']),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with \"None\" in the col_type \n",
    "sentences_df = sentences_df[sentences_df[\"col_type\"].notnull()]\n",
    "#sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_processed_sent</th>\n",
       "      <th>male_count</th>\n",
       "      <th>female_count</th>\n",
       "      <th>apicall_fail</th>\n",
       "      <th>sentences</th>\n",
       "      <th>article_id</th>\n",
       "      <th>year</th>\n",
       "      <th>col_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>[start, new, weekly, slot, appear, watershed, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Bill is to start its new weekly slot, appe...</td>\n",
       "      <td>558151</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>[play, say, new, post, watershed, slot, exciti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sam Callis, who plays Sergeant Callum Stone, s...</td>\n",
       "      <td>558151</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>[term, storytelling, opportunity, give, allow,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"In terms of the storytelling, and the opportu...</td>\n",
       "      <td>558151</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>[still, get, flavour, tighter, sharper]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It's still The Bill, it's got that flavour to...</td>\n",
       "      <td>558151</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>[play, say, drama, look, really, grown, quite,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Alex Walkinshaw, who plays Smithy (Inspector ...</td>\n",
       "      <td>558151</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9706</th>\n",
       "      <td>[organisers, say, extend, programme, live, tou...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Organisers of the X Factor have said they've e...</td>\n",
       "      <td>1043733</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>[vote, judge, week, seven, competition]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jedward were voted off by the X Factor judges ...</td>\n",
       "      <td>1043733</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708</th>\n",
       "      <td>[two, month, tour, begin, see, extra, date, add]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The two month tour, which begins in Liverpool ...</td>\n",
       "      <td>1043733</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>[artists, confirm, tour, contestant]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Artists confirmed for the tour are contestants...</td>\n",
       "      <td>1043733</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9711</th>\n",
       "      <td>[pmliverpool, pmbirmingham, pmbirmingham, pmbi...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Liverpool Arena - 15 February - 7.30pmLiverpoo...</td>\n",
       "      <td>1043733</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     pre_processed_sent  male_count  \\\n",
       "4878  [start, new, weekly, slot, appear, watershed, ...           1   \n",
       "4885  [play, say, new, post, watershed, slot, exciti...           1   \n",
       "4887  [term, storytelling, opportunity, give, allow,...           1   \n",
       "4888            [still, get, flavour, tighter, sharper]           1   \n",
       "4891  [play, say, drama, look, really, grown, quite,...           1   \n",
       "...                                                 ...         ...   \n",
       "9706  [organisers, say, extend, programme, live, tou...           2   \n",
       "9707            [vote, judge, week, seven, competition]           1   \n",
       "9708   [two, month, tour, begin, see, extra, date, add]           3   \n",
       "9709               [artists, confirm, tour, contestant]           2   \n",
       "9711  [pmliverpool, pmbirmingham, pmbirmingham, pmbi...           1   \n",
       "\n",
       "      female_count  apicall_fail  \\\n",
       "4878             0             0   \n",
       "4885             0             1   \n",
       "4887             0             0   \n",
       "4888             0             0   \n",
       "4891             0             1   \n",
       "...            ...           ...   \n",
       "9706             0             0   \n",
       "9707             0             0   \n",
       "9708             0             0   \n",
       "9709             1             0   \n",
       "9711             4             0   \n",
       "\n",
       "                                              sentences  article_id  year  \\\n",
       "4878  The Bill is to start its new weekly slot, appe...      558151  2000   \n",
       "4885  Sam Callis, who plays Sergeant Callum Stone, s...      558151  2000   \n",
       "4887  \"In terms of the storytelling, and the opportu...      558151  2000   \n",
       "4888  \"It's still The Bill, it's got that flavour to...      558151  2000   \n",
       "4891  \"Alex Walkinshaw, who plays Smithy (Inspector ...      558151  2000   \n",
       "...                                                 ...         ...   ...   \n",
       "9706  Organisers of the X Factor have said they've e...     1043733  2000   \n",
       "9707  Jedward were voted off by the X Factor judges ...     1043733  2000   \n",
       "9708  The two month tour, which begins in Liverpool ...     1043733  2000   \n",
       "9709  Artists confirmed for the tour are contestants...     1043733  2000   \n",
       "9711  Liverpool Arena - 15 February - 7.30pmLiverpoo...     1043733  2000   \n",
       "\n",
       "     col_type  \n",
       "4878        1  \n",
       "4885        1  \n",
       "4887        1  \n",
       "4888        1  \n",
       "4891        1  \n",
       "...       ...  \n",
       "9706        1  \n",
       "9707        1  \n",
       "9708        1  \n",
       "9709        1  \n",
       "9711        0  \n",
       "\n",
       "[1520 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1 = df_sample_1.dropna(subset = [\"col_type\"])\n",
    "df_sample_2 = df_sample_2.dropna(subset = [\"col_type\"])\n",
    "df_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1177\n",
       "0     630\n",
       "Name: col_type, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_1[\"col_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class balancing and save as fully preprocessed. make it so the whole notebook runs per year.\n",
    "#TODO seperate out the name function. save two copies, with names and ready for model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLANDA SAMPLE DATA FOR LOG REG\n",
    "result['col_type'] = result.apply(lambda row: compare_count(row['male_count'], row['female_count']),axis=1)\n",
    "#results_df_09 = result[result[\"col_type\"].notnull()]\n",
    "sample_results_df_09 = result.dropna(subset = [\"col_type\"])\n",
    "sample_results_df_09\n",
    "sample_results_df_09.to_pickle('sample_results_df_09.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
